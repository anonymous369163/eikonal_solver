"""
Finetune SAMRoute map_decoder on Gen_dataset_V2/Gen_dataset (multi-image batch training).

Optimized for multi-GPU (DDP), large batch, TF32, cached features, high-throughput data loading.
Supports Single-Region Overfitting for debugging.
"""
import argparse
import os
import glob
import torch
import numpy as np
import tifffile
from PIL import Image
from torch.utils.data import Dataset, DataLoader

# å¼€å¯ TF32ï¼š4090 ç­‰ Ada æ¶æ„ä¸ŠçŸ©é˜µä¹˜æ³•å¯åŠ é€Ÿ 2~3 å€ï¼Œå‡ ä¹æ— ç²¾åº¦æŸå¤±
torch.set_float32_matmul_precision("high")

try:
    import lightning.pytorch as pl
    from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor
    from lightning.pytorch.loggers import TensorBoardLogger
except ImportError:
    import pytorch_lightning as pl
    from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
    from pytorch_lightning.loggers import TensorBoardLogger

from model import SAMRoute
from dataset import build_dataloaders

# ==========================================
# 1. è®­ç»ƒé…ç½®
# ==========================================
class TrainConfig:
    def __init__(self):
        self.SAM_VERSION = 'vit_b'
        self.PATCH_SIZE = 512
        self.NO_SAM = False
        self.USE_SAM_DECODER = False
        self.ENCODER_LORA = False
        self.LORA_RANK = 4   # ä»…å½“ ENCODER_LORA=True æ—¶ç”Ÿæ•ˆ
        self.FREEZE_ENCODER = True   # ç»å¯¹å†»ç»“ SAM Encoderï¼Œåªè®­ç»ƒ Decoderï¼ˆLoRA æ¨¡å¼å¿…é¡» Falseï¼‰
        self.FOCAL_LOSS = False
        self.TOPONET_VERSION = 'default'

        _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
        _PROJECT_ROOT = os.path.normpath(os.path.join(_SCRIPT_DIR, ".."))
        self.SAM_CKPT_PATH = os.path.join(_PROJECT_ROOT, "sam_road_repo", "sam_ckpts", "sam_vit_b_01ec64.pth")

        # è·¯ç”±/æŸå¤±é…ç½®
        self.ROUTE_COST_MODE = 'add'
        self.ROAD_POS_WEIGHT = 5.0
        
        # ğŸš€ æ ¸å¿ƒæ”¹åŠ¨ï¼šå¼€å¯åŒç›®æ ‡æŸå¤±ï¼ŒåŠ å¤§ Dice æƒé‡ï¼Œé€¼è¿«æ¨¡å‹é¢„æµ‹æç»†è´´è¾¹çš„é“è·¯
        self.ROAD_DICE_WEIGHT = 0.8   # æé«˜ä»¥åŠ å¼ºç»†è·¯çº¦æŸï¼ˆåŸ 0.4ï¼Œç»†è·¯æ¼ç”»æ—¶å°è¯• 0.6~0.7ï¼‰
        self.ROAD_DUAL_TARGET = True
        self.ROAD_THIN_BOOST = 10.0    # BCE ç»†è·¯åƒç´ é¢å¤–æƒé‡å€æ•°ï¼ˆåŸ 3.0ï¼‰

        self.ROUTE_LAMBDA_SEG = 1.0
        self.ROUTE_LAMBDA_DIST = 0.0
        self.BASE_LR = 5e-4  # é…åˆ batch_size=16ï¼Œè¾ƒ 4 æ—¶é€‚å½“æ”¾å¤§
        self.ENCODER_LR_FACTOR = 0.1   # configure_optimizers éœ€æ­¤å±æ€§
        self.LR_MILESTONES = [150] 

    def get(self, key, default):
        return getattr(self, key, default)

# ==========================================
# 2. å•å›¾è¿‡æ‹Ÿåˆä¸“å± Dataset (æé€Ÿç‰ˆ)
# ==========================================
class SingleRegionDataset(Dataset):
    def __init__(self, region_dir, config, samples_per_epoch=200):
        self.patch_size = config.PATCH_SIZE
        self.steps = samples_per_epoch
        self.config = config
        
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½å•åŒºåŸŸæ•°æ®: {region_dir}")
        # 1. åŠ è½½ TIF
        tifs = glob.glob(os.path.join(region_dir, "crop_*.tif"))
        img_array = tifffile.imread(tifs[0])
        if img_array.ndim == 3 and img_array.shape[2] >= 3:
            img_array = img_array[:, :, :3]
        elif img_array.ndim == 3 and img_array.shape[0] >= 3: 
            img_array = img_array[:3, :, :].transpose(1, 2, 0)
        if img_array.dtype == np.uint16:
            img_array = (img_array / 256.0).astype(np.uint8)
        else:
            img_array = img_array.astype(np.uint8)
        self.img_array = img_array

        # 2. åŠ è½½ Masks (æ”¯æŒåŒç›®æ ‡)
        masks_thick = glob.glob(os.path.join(region_dir, "roadnet_normalized_*.png"))
        masks_thin = glob.glob(os.path.join(region_dir, "roadnet_*.png"))
        masks_thin = [m for m in masks_thin if "normalized" not in m]
        
        # BCE å¯»æ‰¾å®½æ©ç  (åšåº¦ä¿è¯é«˜å¬å›)
        mask_path = masks_thick[0] if masks_thick else masks_thin[0]
        self.mask_array = np.array(Image.open(mask_path).convert('L'))
        
        # Dice å¯»æ‰¾ç»†æ©ç  (ç»†åº¦ä¿è¯é«˜å®šä½)
        if config.ROAD_DUAL_TARGET and masks_thin:
            self.thin_mask_array = np.array(Image.open(masks_thin[0]).convert('L'))
        else:
            self.thin_mask_array = None

        # 3. åŠ è½½ç¼“å­˜ç‰¹å¾ï¼ˆLoRA æ¨¡å¼ä¸‹å¿…é¡»åœ¨çº¿è®¡ç®—ï¼Œä¸å¯ç”¨ç¼“å­˜ï¼‰
        feats = glob.glob(os.path.join(region_dir, "samroad_feat_full_*.npy"))
        if getattr(config, 'ENCODER_LORA', False):
            self.feat_array = None
            print("LoRA æ¨¡å¼ï¼šè·³è¿‡ cached_featureï¼Œåœ¨çº¿è®¡ç®— Encoder è¾“å‡º")
        elif feats:
            self.feat_array = np.load(feats[0]).astype(np.float32)
            print("âœ“ å·²æ‰¾åˆ° cached_featureï¼Œå¼€å¯å… Encoder æé€Ÿå¾®è°ƒ")
        else:
            self.feat_array = None
            print("âš ï¸ æœªæ‰¾åˆ° cached_featureï¼Œå°†åœ¨çº¿è·‘ Encoder")

    def __len__(self):
        return self.steps

    def __getitem__(self, idx):
        h, w = self.img_array.shape[:2]
        ps = self.patch_size
        
        y = np.random.randint(0, max(1, h - ps))
        x = np.random.randint(0, max(1, w - ps))

        # ğŸš€ æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶å¯¹é½åˆ° 16 çš„æ•´æ•°å€ï¼Œå½»åº•æ¶ˆç­ç‰¹å¾å›¾ä¸ Mask çš„ç©ºé—´é”™ä½ï¼
        y = (y // 16) * 16
        x = (x // 16) * 16
            
        img_crop = self.img_array[y : y + ps, x : x + ps]
        mask_crop = self.mask_array[y : y + ps, x : x + ps]
        
        # å°ºå¯¸è¡¥é½
        pad_y = max(0, ps - img_crop.shape[0])
        pad_x = max(0, ps - img_crop.shape[1])
        if pad_y > 0 or pad_x > 0:
            img_crop = np.pad(img_crop, ((0, pad_y), (0, pad_x), (0, 0)))
            mask_crop = np.pad(mask_crop, ((0, pad_y), (0, pad_x)))

        sample = {
            'rgb': torch.tensor(img_crop, dtype=torch.float32),
            'road_mask': (torch.tensor(mask_crop, dtype=torch.float32) > 0).float()
        }

        if self.thin_mask_array is not None:
            thin_crop = self.thin_mask_array[y : y + ps, x : x + ps]
            if pad_y > 0 or pad_x > 0:
                thin_crop = np.pad(thin_crop, ((0, pad_y), (0, pad_x)))
            sample['road_mask_thin'] = (torch.tensor(thin_crop, dtype=torch.float32) > 0).float()

        if self.feat_array is not None:
            stride = 16
            feat_size = ps // stride
            fy, fx = y // stride, x // stride
            feat_crop = self.feat_array[:, fy : fy + feat_size, fx : fx + feat_size]
            pad_fy = max(0, feat_size - feat_crop.shape[1])
            pad_fx = max(0, feat_size - feat_crop.shape[2])
            if pad_fy > 0 or pad_fx > 0:
                feat_crop = np.pad(feat_crop, ((0, 0), (0, pad_fy), (0, pad_fx)))
            sample['encoder_feat'] = torch.tensor(feat_crop)

        return sample

# ==========================================
# 3. è®­ç»ƒæµç¨‹ (ä½¿ç”¨ Lightning Trainer)
# ==========================================
def train(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"å‡†å¤‡è®­ç»ƒ... æ£€æµ‹åˆ°è®¾å¤‡: {device}")

    _SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
    _PROJECT_ROOT = os.path.normpath(os.path.join(_SCRIPT_DIR, ".."))

    data_root = args.data_root if os.path.isabs(args.data_root) else os.path.join(_PROJECT_ROOT, args.data_root)
    PRETRAINED_CKPT = args.pretrained_ckpt if os.path.isabs(args.pretrained_ckpt) else os.path.join(_PROJECT_ROOT, args.pretrained_ckpt)
    output_dir = args.output_dir if os.path.isabs(args.output_dir) else os.path.join(_PROJECT_ROOT, args.output_dir)
    ckpt_dir = os.path.join(output_dir, "checkpoints")

    os.makedirs(ckpt_dir, exist_ok=True)

    config = TrainConfig()
    if args.lr is not None:
        config.BASE_LR = args.lr
    if args.encoder_lora:
        config.ENCODER_LORA = True
        config.LORA_RANK = args.lora_rank
        config.FREEZE_ENCODER = False  # é¿å…å¤–éƒ¨å†»ç»“è¯¯ä¼¤ LoRA å‚æ•°
        print(f"LoRA æ¨¡å¼: rank={config.LORA_RANK}")
    if args.road_pos_weight is not None:
        config.ROAD_POS_WEIGHT = args.road_pos_weight
    if args.road_dice_weight is not None:
        config.ROAD_DICE_WEIGHT = args.road_dice_weight
    if args.road_thin_boost is not None:
        config.ROAD_THIN_BOOST = args.road_thin_boost
    model = SAMRoute(config)

    # æ‰‹åŠ¨å†»ç»“æœ¬é˜¶æ®µä¸å‚ä¸ Loss çš„å‚æ•°ï¼Œæ»¡è¶³ DDP æ ¡éªŒï¼Œä»è€Œä½¿ç”¨æ ‡å‡† ddp è€Œéé¾Ÿé€Ÿ find_unused_parameters
    if config.FREEZE_ENCODER:
        for p in model.image_encoder.parameters():
            p.requires_grad = False
    if config.ROUTE_LAMBDA_DIST == 0.0:
        model.cost_log_alpha.requires_grad = False
        model.cost_log_gamma.requires_grad = False
        model.eik_gate_logit.requires_grad = False
        if hasattr(model, 'topo_net'):
            for p in model.topo_net.parameters():
                p.requires_grad = False

    # 1. å®‰å…¨åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆè‹¥æ¨¡å‹ç»“æ„å˜åŒ–å¯¼è‡´æ— æ³•åŠ è½½ï¼Œåˆ™ä»éšæœºåˆå§‹åŒ–ç»§ç»­è®­ç»ƒï¼‰
    if os.path.isfile(PRETRAINED_CKPT):
        try:
            print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {PRETRAINED_CKPT}")
            ckpt = torch.load(PRETRAINED_CKPT, map_location='cpu', weights_only=False)
            state_dict = ckpt.get("state_dict", ckpt)
            clean_state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}
            model.load_state_dict(clean_state_dict, strict=False)
            print("âœ“ é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥ï¼ˆå¯èƒ½ä¸ map_decoder ç»“æ„å˜æ›´ä¸å…¼å®¹ï¼‰: {e}")
            print("   å°†ä»éšæœºåˆå§‹åŒ– Decoder å¼€å§‹è®­ç»ƒ")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡ {PRETRAINED_CKPT}ï¼Œä»éšæœº Decoder å¼€å§‹è®­ç»ƒã€‚")

    # 2. æ„å»º DataLoader (è‡ªåŠ¨è·¯ç”±ï¼šå•å›¾æµ‹è¯• æˆ– å…¨é‡è®­ç»ƒ)
    if args.single_region_dir:
        print(f"\n=============================================")
        print(f"âš ï¸ æ³¨æ„ï¼šå½“å‰å¤„äºã€å•å›¾è¿‡æ‹Ÿåˆæµ‹è¯•æ¨¡å¼ã€‘")
        print(f"=============================================\n")
        train_ds = SingleRegionDataset(args.single_region_dir, config, samples_per_epoch=200)
        val_ds = SingleRegionDataset(args.single_region_dir, config, samples_per_epoch=20)
        train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)
        val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)
    else:
        if not os.path.isdir(data_root):
            raise FileNotFoundError(f"æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {data_root}")
        print("æ­£åœ¨æ„å»ºå…¨é‡æ•°æ®é›†å¹¶åŠ è½½åˆ°å†…å­˜...")
        train_loader, val_loader = build_dataloaders(
            root_dir=data_root,
            patch_size=config.PATCH_SIZE,
            batch_size=args.batch_size,
            num_workers=args.workers,
            include_dist=False,
            val_fraction=args.val_fraction,
            samples_per_region=args.samples_per_region,
            use_cached_features=args.use_cached_features,
            preload_to_ram=args.preload_to_ram,
            road_dilation_radius=args.road_dilation_radius,
        )

    # 3. é…ç½® Lightning å›è°ƒ
    ckpt_filename = f'best_{args.run_name}' if args.run_name else 'best'
    checkpoint_callback = ModelCheckpoint(
        dirpath=ckpt_dir,
        filename=ckpt_filename,
        save_top_k=1,
        monitor='val_seg_loss',
        mode='min',
        save_last=True,
        every_n_epochs=1,
    )
    lr_monitor = LearningRateMonitor(logging_interval='epoch')

    # è®­ç»ƒæ—¥å¿—ï¼ˆTensorBoardï¼‰
    tb_logger = TensorBoardLogger(save_dir=output_dir, name="tensorboard", version=None)

    # 4. å¯åŠ¨ Trainer
    use_gpu = torch.cuda.is_available()
    n_gpus = torch.cuda.device_count() if use_gpu else 0
    devices = args.devices if args.devices is not None else (n_gpus or 1)
    use_ddp = use_gpu and devices > 1
    precision = "16-mixed" if use_gpu else "32"
    strategy = "ddp" if use_ddp else "auto"
    
    trainer = pl.Trainer(
        max_epochs=args.epochs,
        accelerator="gpu" if use_gpu else "cpu",
        devices=devices,
        strategy=strategy,
        precision=precision,
        logger=tb_logger,
        callbacks=[checkpoint_callback, lr_monitor],
        log_every_n_steps=10 if args.single_region_dir else 50, # å•å›¾æ¨¡å¼æ‰“å°å‹¤ä¸€ç‚¹
        val_check_interval=1.0 if args.single_region_dir else 0.25, # å•å›¾æ¯epochéªŒè¯
        enable_progress_bar=True,
        enable_model_summary=False,
    )
    
    if use_ddp:
        print(f"ä½¿ç”¨ {devices} å¼  GPU è¿›è¡Œ DDP åˆ†å¸ƒå¼è®­ç»ƒ")

    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.fit(model, train_loader, val_loader)
    print(f"\nâœ… è®­ç»ƒå®Œæˆã€‚æœ€ä½³æƒé‡å·²ä¿å­˜åœ¨: {ckpt_dir}")
    print(f"   è®­ç»ƒæ›²çº¿: tensorboard --logdir {os.path.join(output_dir, 'tensorboard')} --port 6006")

# ==========================================
# 4. å‘½ä»¤è¡Œå‚æ•°
# ==========================================
def parse_args():
    p = argparse.ArgumentParser(
        description="SAMRoute finetune â€” å¤šå›¾æ‰¹é‡æ­£å¼è®­ç»ƒ (æ”¯æŒ DDPã€TF32ã€cached features)",
        epilog="å•å›¾è¿‡æ‹Ÿåˆæµ‹è¯•: python finetune_demo.py --single_region_dir path/to/region --epochs 100"
    )
    p.add_argument("--data_root", default="Gen_dataset_V2/Gen_dataset", help="æ•°æ®é›†æ ¹ç›®å½•")
    
    # ğŸ¯ å•å›¾æµ‹è¯•å¼€å…³
    p.add_argument("--single_region_dir", type=str, default=None, 
                   help="å¦‚æœæä¾›å•ä¸ªå›¾åƒçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆåŒ…å« .tif å’Œ .pngï¼‰ï¼Œåˆ™æ— è§† data_rootï¼Œåªè®­ç»ƒè¿™ä¸€ä¸ªåŒºåŸŸï¼")
                   
    p.add_argument("--pretrained_ckpt", default="checkpoints/cityscale_vitb_512_e10.ckpt", help="é¢„è®­ç»ƒ SAM-Road æƒé‡è·¯å¾„")
    p.add_argument("--output_dir", default="training_outputs/finetune_demo", help="è¾“å‡ºç›®å½•")
    p.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    p.add_argument("--batch_size", type=int, default=32, help="batch sizeï¼Œcached æ¨¡å¼æ˜¾å­˜å ç”¨ä½å¯å¼€ 32~64")
    p.add_argument("--lr", type=float, default=None, help="å­¦ä¹ ç‡ï¼Œé»˜è®¤ 5e-4 (é…åˆ batch 16)")
    p.add_argument("--val_fraction", type=float, default=0.1, help="éªŒè¯é›†åŸå¸‚å æ¯” (0~1)")
    p.add_argument("--samples_per_region", type=int, default=50, help="æ¯åŒºåŸŸæ¯ epoch é‡‡æ ·æ•°")
    p.add_argument("--road_dilation_radius", type=int, default=3, help="å½’ä¸€åŒ– mask åŠå¾„")
    p.add_argument("--workers", type=int, default=4, help="DataLoader workersï¼Œpreload æ—¶ 4 è¶³å¤Ÿ")
    p.add_argument("--devices", type=int, default=1, help="GPU æ•°é‡ï¼Œé»˜è®¤å•å¡è®­ç»ƒ")
    p.add_argument("--use_cached_features", action="store_true", help="ä½¿ç”¨é¢„è®¡ç®— samroad_feat_full_*.npy è·³è¿‡ Encoder")
    p.add_argument("--no_cached_features", action="store_true", help="å…³é—­ cached featuresï¼Œå§‹ç»ˆè·‘ Encoder")
    p.add_argument("--no_preload", action="store_true", help="å…³é—­ preload_to_ram")
    p.add_argument("--fast", action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼šå•å¡ batch32 workers4ï¼Œä¾¿äºè°ƒè¯•")
    # å‚æ•°æ‰«æï¼šROAD_POS_WEIGHT / ROAD_DICE_WEIGHT / ROAD_THIN_BOOST
    p.add_argument("--road_pos_weight", type=float, default=None, help="BCE æ­£æ ·æœ¬æƒé‡")
    p.add_argument("--road_dice_weight", type=float, default=None, help="Dice æŸå¤±æƒé‡ï¼Œè¶Šå¤§è¶Šå¼ºè°ƒç»†è·¯")
    p.add_argument("--road_thin_boost", type=float, default=None, help="BCE ç»†è·¯åƒç´ é¢å¤–æƒé‡å€æ•° (é»˜è®¤ 6.0)")
    p.add_argument("--run_name", type=str, default=None, help="å‚æ•°æ‰«ææ—¶æŒ‡å®šï¼Œcheckpoint ä¿å­˜ä¸º best_{run_name}.ckpt")
    # LoRA  encoder å¾®è°ƒ
    p.add_argument("--encoder_lora", action="store_true", help="å¼€å¯ LoRA å¾®è°ƒ Encoder")
    p.add_argument("--lora_rank", type=int, default=4, help="LoRA rankï¼Œä»… --encoder_lora æ—¶ç”Ÿæ•ˆ")
    args = p.parse_args()
    args.preload_to_ram = not args.no_preload
    args.use_cached_features = args.use_cached_features and not args.no_cached_features
    if args.encoder_lora:
        args.use_cached_features = False  # LoRA å¿…é¡»åœ¨çº¿è®¡ç®—ï¼Œä¸å¯ç”¨ç¼“å­˜
    if args.fast:
        args.devices = args.devices or 1
        args.batch_size = 32
        args.workers = 4
        args.use_cached_features = True
        print("âš ï¸ --fast: å•å¡ batch=32 workers=4 use_cached_features=True")
    return args

if __name__ == "__main__":
    args = parse_args()
    train(args)